import json
import os
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.markdown import Markdown
from rich.theme import Theme
from rich.live import Live
from rich.text import Text

class PentestAgent:
    """
    Agent to handle pentest tasks.
    """
    def __init__(self, client, data_dir="./logs/", task_id=None):
        self.client = client
        self.data_dir = data_dir
        self.task_id = task_id
        
        # Define a custom theme with more subdued colors
        custom_theme = Theme({
            "markdown": "",
            "markdown.heading": "",
            "markdown.code": "",
            "markdown.pre": "",
            "markdown.link": "",
            "markdown.list": "",
            "markdown.strong": "",
            "markdown.emphasis": "",
            "markdown.block_quote": "",
            "repr.number": "",
            "repr.string": "",
            "repr.string_quote": "",
            "markdown.table": "",
            "markdown.table.header": "",
            "markdown.table.row": "",
            "markdown.table.cell": "",
            "markdown.hrule": ""
        })
        self.console = Console(theme=custom_theme)
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)

    def get_history_file_path(self):
        return os.path.join(self.data_dir, f"history_{self.task_id}.json")

    def load_history(self):
        history_file = self.get_history_file_path()
        if not os.path.exists(history_file):
            return []

        with open(history_file, "r") as file:
            return json.load(file)

    def save_history(self, history):
        history_file = self.get_history_file_path()
        with open(history_file, "w") as file:
            json.dump(history, file, indent=4)

    def set_task(self, task):
        """
        Set the task to be worked on to the history.
        """
        self.task = task
        history = self.load_history()
        history.append({"role": "user", "content": task})
        self.save_history(history)



    def generate_thought(self):
        """
        Generate a thought about the next step.
        """
        # Load history
        history = self.load_history()

        # Load the proper prompt based on whether this is the first step or not
        if len(history) == 1:
            from app.prompts import PENTEST_FIRST_STEP_THOUGHT_PROMPT
            instruction = PENTEST_FIRST_STEP_THOUGHT_PROMPT
        else:
            from app.prompts import PENTEST_FOLLOWUP_THOUGHT_PROMPT
            instruction = PENTEST_FOLLOWUP_THOUGHT_PROMPT

        messages = [{"role": "system", "content": instruction}] + history

        # Process the task using the AI model
        completion = self.client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            stream=True
        )

        # Initialize a panel with no content
        self.console.print(Text("Thought", style="bold green"))
        thought_panel = Panel("", border_style="green", expand=False)
        with Live(thought_panel, console=self.console, refresh_per_second=10) as live:
            # Start the live display

            full_message_content = ''

            # Begin the streaming completion with OpenAI
            completion = self.client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages,
                stream=True
            )

            for chunk in completion:
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    message_content = delta.content
                    if message_content:
                        # Update the full message content
                        full_message_content += message_content
                        
                        # Update the live output with the new content
                        thought_markdown = Markdown(full_message_content)
                        live.update(Panel(thought_markdown, border_style="green", expand=False))

        # Save to history
        history.append({"role": "assistant", "content": full_message_content})
        self.save_history(history)

        return full_message_content

    def determine_next_action(self, thought):
        """
        Determine the next action based on the thought.
        """
        include_command = self.include_command(thought)
        if not include_command:
            is_job_successful = self.is_job_successful()
            if is_job_successful:
                return None, 'success'
            else:
                return None, 'failure'

        next_action = self.extract_command(thought)
        return next_action, 'continue'

    def extract_command(self, thought):
        """
        Uses an OpenAI GPT model to extract a command from the AI's response.
        """
        # Prepare the prompt for GPT model
        from app.prompts import THOUGHT_TO_COMMAND_PROMPT
        prompt = THOUGHT_TO_COMMAND_PROMPT

        self.console.print(Text("Command", style="bold cyan"))
        command_panel = Panel("", border_style="cyan", expand=False)
        with Live(command_panel, console=self.console, refresh_per_second=10) as live:
            # Begin the streaming completion with OpenAI
            completion = self.client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=[{"role": "system", "content": prompt}] + [{"role": "user", "content": thought}],
                stream=True
            )

            full_command_content = ''
            for chunk in completion:
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    command_content = delta.content
                    if command_content:
                        # Update the full command content
                        full_command_content += command_content
                        
                        # Update the live output with the new content
                        command_syntax = Syntax(full_command_content, "bash", theme="monokai", word_wrap=True)
                        live.update(Panel(command_syntax, border_style="cyan", expand=False))

        return full_command_content

    def is_job_successful(self):
        """
        Uses an OpenAI GPT model to determine if the job is succesfully completed based on the AI's response.
        """
        # Load history
        history = self.load_history()

        # Turn the history into a string
        history_string = ""
        for message in history:
            if message["role"] == "user":
                history_string += f"User: {message['content']}\n"
            elif message["role"] == "assistant":
                history_string += f"Assistant: {message['content']}\n"
            elif message["role"] == "system":
                history_string += f"System: {message['content']}\n"


        # Prepare the prompt for GPT model
        prompt = f"An AI assitant is helping a penetration tester work on his job. At each iteration, the AI gives the penetration tester some ideas for him to try. Here's the history of the task so far:\n\n{history_string}\n\n The task that the penetration tester is trying to do is described as follows: \n\n{self.task}\n\n Based on the history and the task, determine if the task is already successfully finished. \n\n Is the task successfully finished? Respond 'yes' if the task is successful, respond 'no' if the task is failed or is still ongoing. Give me a clear 'yes' or 'no', don't say additional words."

        # Call the GPT model
        completion = self.client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[{"role": "system", "content": "You are a helpful assistant"}]+[{"role": "user", "content": prompt}]
        )

        # Extract and interpret the model's response
        model_response = completion.choices[0].message.content.lower()

        if "yes" in model_response.lower():
            return True
        elif "no" in model_response.lower():
            return False

    def include_command(self, thought):
        """
        Uses an OpenAI GPT model to determine if the AI's response includes a command line code.
        """
        # Prepare the prompt for GPT model
        prompt = f"An AI assitant is helping a penetration tester work on his job. At each iteration, the AI gives the penetration tester some ideas for him to try. At some point, the AI gives the human the following response\n\nResponse: '{thought}'\n\n Determine if there's any command line code that can be executed in the response. \n\n Is there any command line code that can be executed in the response? Respond 'yes' if there is, respond 'no' there's no clear command line code in the response. Give me a clear 'yes' or 'no', don't say additional words."

        # Call the GPT model
        completion = self.client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=[{"role": "system", "content": "You are a helpful assistant"}]+[{"role": "user", "content": prompt}]
        )

        # Extract and interpret the model's response
        model_response = completion.choices[0].message.content.lower()

        if "yes" in model_response:
            return True
        elif "no" in model_response:
            return False

    def generate_summary(self, success):
        """
        Generate a summary of the steps taken.
        """
        # Load history
        history = self.load_history()

        # Turn the history into a string
        history_string = "Here are all the steps that I've taken during this task:\n\n"
        for message in history:
            if message["role"] == "user":
                history_string += f"User: {message['content']}\n"
            elif message["role"] == "assistant":
                history_string += f"Assistant: {message['content']}\n"
            elif message["role"] == "system":
                history_string += f"System: {message['content']}\n"
        history_string += "\n\n"
        history_string += "The summary of this task is as follows:\n\n"

        # Load the proper prompt based on whether the task is successful or not
        if success:
            from app.prompts import SUCCESS_SUMMARY_PROMPT
            instruction = SUCCESS_SUMMARY_PROMPT
        else:
            from app.prompts import FAILURE_SUMMARY_PROMPT
            instruction = FAILURE_SUMMARY_PROMPT

        messages = [{"role": "system", "content": instruction}] + [{"role": "user", "content": history_string}]

        # Process the task using the AI model
        completion = self.client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            stream=True
        )

        # Initialize a panel with no content
        self.console.print(Text("Summary", style="bold dark_cyan"))
        summary_panel = Panel("", border_style="bold dark_cyan", expand=False)
        with Live(summary_panel, console=self.console, refresh_per_second=10) as live:
            # Start the live display

            # Begin the streaming completion with OpenAI
            completion = self.client.chat.completions.create(
                model="gpt-4-1106-preview",
                messages=messages,
                stream=True
            )

            full_message_content = ''
            for chunk in completion:
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    message_content = delta.content
                    if message_content:
                        # Update the full message content
                        full_message_content += message_content
                        
                        # Update the live output with the new content
                        summary_markdown = Markdown(full_message_content)
                        live.update(Panel(summary_markdown, border_style="bold dark_cyan", expand=False))

